fit[["scale"]]
sink()
summary(fit)                                  # Print the variance accounted for
fit[["loadings"]]
sink()
sssaas
sa
print"whatsup"
print("hello")
# 7 PCs can explain >75% of variance in data
loadings(fit)                                 # Show the principal component loadings
data = read.csv("/Users/hasansulaeman/PCAscoresAndData.csv")
data = read.csv("Desktop/wtf.csv")
table(data$SwabCollector)
sasa = table(data$SwabCollector)
view(sasa)
open(sasa)
sasa = as.data.frame(sasa)
View(sasa)
Zsmap
library(ggplot2)
library(ggmap)
library(maps)
library(mapdata)
data(worldMapEnv)
pos = subset(data, data$BdStatus == "Positive")
p = ggplot() + geom_polygon(data=w2hr, aes(x=long, y = lat, group = group), fill=NA, color="black") +
coord_fixed(1.3) + theme_nothing()
w2hr = map_data("world")
p = ggplot() + geom_polygon(data=w2hr, aes(x=long, y = lat, group = group), fill=NA, color="black") +
coord_fixed(1.3) + theme_nothing()
Zsmap = p + geom_point(data = Zs, aes(Zs$lon, Zs$lat), size = 2, alpha=0.5, color="red") +
labs(title="1910 or Before")
Zsmap
Zsmap = p + geom_point(data = Zs, aes(Zs$lon, Zs$lat), size = 2, alpha=0.5, color="red") +
labs(title="1910 or Before")
Zs = subset(data, Zswab > 100)
Zsmap = p + geom_point(data = Zs, aes(Zs$lon, Zs$lat), size = 2, alpha=0.5, color="red") +
labs(title="1910 or Before")
Zsmap
View(Zs)
Zsmap = p + geom_point(data = Zs, aes(Zs$Longitude, Zs$Latitude), size = 2, alpha=0.5, color="red") +
labs(title="1910 or Before")
Zsmap
tot = subset(pos, Year < 1981 | Zswab > 100)
totmap = p + geom_point(data = tot, aes(tot$lon, tot$lat), size = 2, alpha=0.5, color="red") +
labs(title="1910 or Before")
totmap
totmap = p + geom_point(data = tot, aes(tot$Longitude, tot$Latitude), size = 2, alpha=0.5, color="red") +
labs(title="1910 or Before")
totmap
write.csv(tot, "sendtobree.csv")
table(tot$Order)
table(tot$Country)
tot = subset(pos, Year < 1981 | Zswab > 50)
totmap = p + geom_point(data = tot, aes(tot$Longitude, tot$Latitude), size = 2, alpha=0.5, color="red") +
labs(title="1910 or Before")
totmap
table(tot$Country)
View(tot)
data = read.csv("Desktop/wtf.csv")
data$lon = data$Longitude
data$lat = data$Latitude
pos = subset(data, data$BdStatus == "Positive")
tot = subset(pos, Year < 1981 | Zswab > 50)
totmap = p + geom_point(data = tot, aes(tot$Longitude, tot$Latitude), size = 2, alpha=0.5, color="red") +
labs(title="1910 or Before")
library(ggplot2)
library(ggmap)
library(maps)
library(mapdata)
data(worldMapEnv)
p = ggplot() + geom_polygon(data=w2hr, aes(x=long, y = lat, group = group), fill=NA, color="black") +
coord_fixed(1.3) + theme_nothing()
geom_point(data = phil, aes(phil$lon, phil$lat, color = as.factor(phil$Year)), size = 1)
w2hr = map_data("world")
p = ggplot() + geom_polygon(data=w2hr, aes(x=long, y = lat, group = group), fill=NA, color="black") +
coord_fixed(1.3) + theme_nothing()
geom_point(data = phil, aes(phil$lon, phil$lat, color = as.factor(phil$Year)), size = 1)
totmap = p + geom_point(data = tot, aes(tot$Longitude, tot$Latitude), size = 2, alpha=0.5, color="red") +
labs(title="1910 or Before")
totmap
data = read.csv("Desktop/sendtobree.csv")
data$lon = data$Longitude
data$lat = data$Latitude
tot = data
totmap = p + geom_point(data = tot, aes(tot$Longitude, tot$Latitude), size = 2, alpha=0.5, color="red") +
labs(title="1910 or Before")
totmap
plot(table(Zs$Year))
plot(table(tot$Year))
plot(table(tot$Year), ylab="n", xlab = "Year")
plot(table(tot$Year), ylab="n", xlab = "Year", main="Samples per Year")
plot(table(tot$Country), ylab="n", xlab = "Year", main="Samples per Country")
plot(table(tot$Country), ylab="n", xlab = "Year", main="Samples per Country", axes=FALSE)
axis(side = 2, lwd = 0, line = -.4, las = 1)
axis(side = 1, lwd = 0, line = -.4, las = 2)
axis(side = 1, las = 2)
plot(table(tot$Order), ylab="n", xlab = "Year", main="Samples per Country")
plot(table(tot$Family), ylab="n", xlab = "Year", main="Samples per Country")
table(tot$Family)
fam = table(tot$Family)
fam = as.data.frame(table(tot$Family))
View(fam)
names(fam) = c("Family", "n")
fam = as.data.frame(table(tot$Order))
names(fam) = c("Order", "n")
fam = as.data.frame(table(tot$Country))
names(fam) = c("Country", "n")
# Data and library setup ----
library(DAAG)
data = read.csv("/Users/hasansulaeman/PCAscoresAndData.csv")
# Binomial logistic regression ----
lm = glm(data.diseaseDet ~ Comp.1 + Comp.2 + Comp.3 + Comp.4 + Comp.5 + Comp.6 +
Comp.7 + Comp.8 + Comp.9 + Comp.10, data=data, family = "binomial")
step.lm = step(lm, direction = "both")
summary(glm(data.diseaseDet ~ Comp.1 + Comp.2 + Comp.3 + Comp.5 + Comp.6 +
Comp.7 + Comp.8 + Comp.9, data=data, family = "binomial"))
samples=read.csv("/Users/hasansulaeman/Dropbox/Africa Bd Project/Africa manuscript/data/
Sonia_Hirschfeldetal_Data_for_GLM_01102018.csv", header=TRUE)
samples=read.csv("/Users/hasansulaeman/Dropbox/Africa Bd Project/Africa manuscript/data/Sonia_Hirschfeldetal_Data_for_GLM_01102018.csv", header=TRUE)
View(samples)
install.packages(sjplot)
install.packages(sjPlot)
install.packages("sjPlot")
library(sjPlot)
# Creating a table for publication ----
sjt.lm(lm, step.lm)
# Creating a table for publication ----
tab.model(lm, step.lm)
?tab_model
step.lm = step(lm, direction = "both")
tab_model(lm, lm2, lm3, step.lm)
# Creating a table for publication ----
lm2 = glm(data.diseaseDet ~ Comp.1 + Comp.2 + Comp.3 + Comp.5 + Comp.6 +
Comp.7 + Comp.8 + Comp.9 + Comp.10, data=data, family = "binomial")
lm3 = glm(data.diseaseDet ~ Comp.1 + Comp.2 + Comp.3 + Comp.4 + Comp.5 +
Comp.6 + Comp.7 + Comp.8 + Comp.9 + Comp.10, data=data, family = "binomial")
tab_model(lm, lm2, lm3, step.lm)
tab_model(lm, lm2, step.lm)
?tab_model
tab_model(lm, lm2, step.lm, show.aic = T, show.ci = F, show.est = T, show.fstat = T)
tab_model(lm, lm2, step.lm, show.aic = T, show.ci = F, show.est = T,
show.fstat = T, show.obs = F)
tab_model(lm, lm2, step.lm, show.aic = T, show.ci = F, show.est = T,
show.fstat = T, show.obs = F, show.r2 = F)
step.lm = step(lm, direction = "both")
summary(glm(data.diseaseDet ~ Comp.1 + Comp.2 + Comp.3 + Comp.5 + Comp.6 +
Comp.7 + Comp.8 + Comp.9, data=data, family = "binomial"))
ale = read.csv("Desktop/Alessandro Museum Swabs - Bd Data.csv")
seq = read.csv("Desktop/HistoricalAtelopusTelmatobius_ZEabove50.csv")
for (i in 1:nrow(seq)) {
x = match(seq[i,]$Specimen, ale$Sample)
print(X)
}
for (i in 1:nrow(seq)) {
x = match(seq[i,]$Specimen, ale$Sample)
print(x)
}
for (i in 1:nrow(seq)) {
which(ale$Sample == seq[i,]$Specimen)
}
View(ale)
View(seq)
names = seq[1:60,]$Specimen
which(ale$Sample == names[[i]])
for (i in 1:nrow(seq)) {
which(ale$Sample == names[[i]])
}
which(names == ale$Sample)
for (i in 1:nrow(seq)) {
which(names == ale$Sample)
}
match(names, ale)
match(names, ale$Sample)
match(y=names, x=ale$Sample)
match(ale$Sample, names)
ale$Sample
match(names, ale$Sample)
?boxplot
cereal = read.csv("/Users/hasansulaeman/Downloads/cereal.csv")
head(cereal)
moarfiber = subset(cereal, fiber > 5)
moarfiber
print(row.names(moarfiber))
?read.csv
# Hylarana chalconota ----
## Work the base
lh = subset(javatrial, species == 2)
km_lh = with(lh, Surv(time, status))
km_lh_fit = survfit(Surv(time, status) ~ treatment, data=lh)
## Plotting
### All 3 groups
LH = autoplot(km_lh_fit, conf.int = FALSE, xlim = c(0, 56), xlab = "Time",
ylab = "Percent Survival", main = "HC all 3 groups")
LH + theme_classic()
### Just 2 groups
HC = subset(lh, lh$arrival == 0)
km_hc = with(HC, Surv(time, status))
km_hc_fit = survfit(Surv(time, status) ~ treatment, data=HC)
H = autoplot(km_hc_fit, conf.int = FALSE, xlim = c(0, 56), xlab = "Time",
ylab = "Percent Survival", main = "HC no Field Positives")
H + theme_classic()
## Cox proportional hazards model, constant covariates
coxph.fit = coxph(Surv(time, status) ~ treatment + as.factor(arrival),
method="breslow", data = lh)
print(coxph.fit)
?sprintf
wwe = read.csv("/Users/hasansulaeman/WWE-Data-2016.csv")
wwe
print(wwe$Wins + wwe$Losses)
print(sum(wwe$Wins + wwe$Losses))
for i in 1:nrow(wwe) {
wwe[i,]$sum = print(sum(wwe$Wins + wwe$Losses))
}
for i in 1:nrow(wwe) {
wwe[i,]$sum = print(sum(wwe[i,]$Wins + wwe[i,]$Losses))
}
for i in 1:nrow(wwe) {
wwe[i,]$sum = sum(wwe[i,]$Wins + wwe[i,]$Losses)
}
View(wwe)
wwe$sum = 0
for i in 1:nrow(wwe) {
wwe[i,]$sum = sum(wwe[i,]$Wins + wwe[i,]$Losses)
}
View(wwe)
for i in 1:nrow(wwe) {
wwe[i,5] = sum(wwe[i,2] + wwe[i,3] + wwe[i,4])
}
for i in 1:nrow(wwe) {
wwe[i,5] = sum(wwe[i,2] + wwe[i,3] + wwe[i,4])}
library(rgl)
read.ply("/Users/hasansulaeman/Desktop/Morphosource_11004_M22671-44010/crotaulus_skull.ply")
rgl::read.ply("/Users/hasansulaeman/Desktop/Morphosource_11004_M22671-44010/crotaulus_skull.ply")
install.packages("geomorph")
library(geomorph)
read.ply("/Users/hasansulaeman/Desktop/Morphosource_11004_M22671-44010/crotaulus_skull.ply")
install.packages("seewave")
install.packages("seewave")
library(seewave)
install.packages(seewave)
install.packages("seewave")
install.packages("seewave")
library(seewave)
install.packages("tuneR")
install.packages("tuneR")
library(tuneR)
?readWave
song = readWave("Documents/DS_lm_A_13.wav.download/DS_lm_A_13.wav")
song
melodyplot(song)
melodyplot(song)
play(song)
play-method(song)
listen(song)
setWavPlayer('/usr/bin/afplay')
listen(song)
listen(song)
listen(song)
?oscillo
?spectro
listen(song)
oscillo(song)
spectro(song)
spectro(song)
listen(song1)
# generate an oscillogram
oscillo(song1)
song1 = readWave("Documents/DS_lm_A_13.wav.download/DS_lm_A_13.wav")
listen(song1)
# generate an oscillogram
oscillo(song1)
# generate a spectogram
spectro(song1)
# generalize mean power spectrum
meanspec(song1)
# Extract the envelope of the song
hilbert(song1)
# Read in two songs
song2 = readWave("Documents/DS_sb_KWRS.wav")
song3 = readWave("Documents/DS_sb_MKRS.wav")
graphs = function(songname) {
listen(songname)
oscillo(songname)
spectro(songname)
meanspec(songname)
hilbert(songname)
}
## Do the exact same for the other two songs as the song above
graph(song2)
## Do the exact same for the other two songs as the song above
graphs(song2)
graphs(song3)
## Temporal cross correlation of the two songs
corspec(song2, song3)
## Temporal cross correlation of the two songs
corspec(spec(song2), spec(song3)
## Temporal cross correlation of the two songs
corspec(spec(song2), spec(song3))
## Temporal cross correlation of the two songs
corspec(spec(song2), spec(song3))
song2
song3
song2
song3 = readWave("Documents/DS_sb_MKRS.wav", from=0, to=2.34)
song3
song3 = cutw(song3, from = 0, to = 2.34)
song3 = readWave("Documents/DS_sb_MKRS.wav")
song3 = cutw(song3, from = 0, to = 2.34)
song3
song2
song2 = cutw(song2, from = 0, to = 2.34)
song2
song2
## Do the exact same for the other two songs as the song above
graphs(song2)
graphs(song3)
## Do the exact same for the other two songs as the song above
graphs(song2)
# Two song comparison
## reading the songs
song2 = readWave("Documents/DS_sb_KWRS.wav", from=0, to=2.34)
song2
# Two song comparison
## reading the songs
song2 = readWave("Documents/DS_sb_KWRS.wav", from=0.1, to=2.34)
song2
?song2
?readWave
song2 = cutw(song2, from=0, to=2.34, output="wav")
# Two song comparison
## reading the songs
song2 = readWave("Documents/DS_sb_KWRS.wav")
song2 = cutw(song2, from=0, to=2.34, output="wav")
song2 = cutw(song2, from=0, to=2.34, output="Wave")
# Two song comparison
## reading the songs
song2 = readWave("Documents/DS_sb_KWRS.wav")
song2 = cutw(song2, from=0, to=2.34, output="Wave")
## Do the exact same for the other two songs as the song above
graphs(song2)
graphs(song3)
## Temporal cross correlation of the two songs
corspec(spec(song2), spec(song3))
song2 = cutw(song2, from=0, to=2.30, output="Wave")
song2 = cutw(song2, from=0, to=2.30, output="Wave")
## Do the exact same for the other two songs as the song above
graphs(song2)
graphs(song3)
## Temporal cross correlation of the two songs
corspec(spec(song2), spec(song3))
# Two song comparison
## reading the songs
song2 = readWave("Documents/DS_sb_KWRS.wav")
song2 = cutw(song2, from=0, to=2.30, output="Wave")
song3 = readWave("Documents/DS_sb_MKRS.wav")
song3 = cutw(song2, from=0, to=2.30, output="Wave")
## Temporal cross correlation of the two songs
corspec(spec(song2), spec(song3))
# Two song comparison
## reading the songs
song2 = readWave("Documents/DS_lm_A_13.wav")
song3 = readWave("Documents/DS_lm_C_13.wav")
# Two song comparison
## reading the songs
song2 = readWave("Documents/DS_lm_A_13.wav")
song3 = readWave("Documents/DS_lm_C_13.wav")
## Do the exact same for the other two songs as the song above
graphs(song2)
## Do the exact same for the other two songs as the song above
graphs(song2, f)
## Temporal cross correlation of the two songs
corspec(spec(song2), spec(song3))
## Temporal cross correlation of the two songs ----
corenv(song2, song3)
?corenv
pr2 = c(19,31, 26, 18, 29, 32, 29, 29, 37, 32, 30, 34, 33, 34, 28, 32, 31, 16, 34, 39)
var(pr2)
?var
summary(pr2)
stdev(pr2)
stdev(pr2)
?sd
sd(pr2)
hist(pr2)
?hist
hist(pr2, breaks=c(12.5, 15, 17.5, 20, 22.5, 25, 27.5, 30, 32.5, 35, 37.5, 40))
load("~/Dropbox/Java Bd/Data/2016_data.csv")
dat = read.csv(file.choose())
View(dat)
BdAsia = subset(dat, sampleID=c(738,700,681,861,657,172,676,693,797,777,721,565,612,759,465,921,920))
?subet
?subset
BdAsia = subset(dat, sampleID=738|700|681|861)
View(BdAsia)
BdAsia = subset(dat, dat$sampleID=c(738,700,681,861,657,172,676,693,797,777,721,565,612,759,465,921,920))
BdAsia = subset(dat, dat$sampleID=738|700)
BdAsia = subset(dat, dat$sampleID = 738|700)
BdAsia = subset(dat, dat$sampleID = 700)
BdAsia = subset(dat, sampleID = 700 | sampleID = 738)
BdAsia = subset(dat, sampleID == 700 | sampleID == 738)
BdAsia = subset(dat, sampleID == 700|738)
BdAsia = subset(dat, sampleID == 700 | sampleID == 738)
BdAsia = subset(dat, sampleID == 700
| sampleID == 738
| sampleID == 681
| sampleID == 861
| sampleID == 657
| sampleID == 657
| sampleID == 172
| sampleID == 676
| sampleID == 693
| sampleID == 797
| sampleID == 777
| sampleID == 721
| sampleID == 565
| sampleID == 612
| sampleID == 759
| sampleID == 465
| sampleID == 921
| sampleID == 920)
BdGPL = subset(dat, sampleID == 428
| sampleID == 732
| sampleID == 737)
BdGPL$lineage = "GPL"
BdAsia$lineage = "Asia"
rbind(BdGPL, BdAsia)
data = rbind(BdGPL, BdAsia)
View(data)
write.csv(data, "lineagemap.csv")
JAMSR = read.csv(file.choose())
binom.test(5,333)$conf.int[1]
binom.test(5,333)$conf.int[2]
binom.test(6,73)$conf.int[1]
binom.test(6,73)$conf.int[2]
binom.test(6,73)$conf.int[2]
len(num)
length(num)
num = c(0,12)
binom.test(num[1],num[2])$conf.int[1]
binom.test(num[1],num[2])$conf.int[2]
num = c(3,13)
binom.test(num[1],num[2])$conf.int[1]
binom.test(num[1],num[2])$conf.int[2]
3/13*100
num = c(2,27)
binom.test(num[1],num[2])$conf.int[1]
binom.test(num[1],num[2])$conf.int[2]
2/27
num = c(1,21)
binom.test(num[1],num[2])$conf.int[1]
binom.test(num[1],num[2])$conf.int[2]
1/21*100
setwd("SierraNevadaBdHistorical/")
iswd
## Reading Data File
MuseumEverything = read.csv(file.choose(), header=TRUE)
## Data Preparation & Cleanup
mu.yr = dcast(data = MuseumEverything, Year~`BdStatus`,
value.var='Decade',fun.aggregate=length)
library(runjags)
library(rjags)
library(reshape2)
## Data Preparation & Cleanup
mu.yr = dcast(data = MuseumEverything, Year~`BdStatus`,
value.var='Decade',fun.aggregate=length)
names(mu.yr)[c(2,3)] = c('neg','pos')
mu.yr$tot = mu.yr$neg+mu.yr$pos
mu.yr$yrc = mu.yr$Year-1900
## Establish Variables
inf = mu.yr$pos
n = mu.yr$tot
time = mu.yr$yrc
nrows = nrow(mu.yr)
# Bayesian Analysis ----
## Create Model
ModelString = 'model{
for(ii in 1:nrows){
inf[ii]~dbin(p[ii], n[ii])
p[ii]<-ppres[ii]*mu
ppres[ii]<-step(tst[ii])
tst[ii]<-time[ii]-arr.time}
arr.time~dunif(-48,60) mu~dbeta(1,1)}'
## Run the model
ArrivalTime = jags.model(textConnection(ModelString),
data = list('inf'=inf, 'n'=n,
'time'=time, 'nrows'=nrows),
n.chains = 3, n.adapt = 10000)
update(ArrivalTime,10000)
ArrivalTimeSamples = coda.samples(arr.time, c('arr.time','mu'),
n.iter=20000)
ArrivalTimeSamples = coda.samples(ArrivalTime, c('arr.time','mu'),
n.iter=20000)
## Plot and Summarize
plot(ArrivalTimeSamples)
summary(ArrivalTimeSamples)
?data.frame
?as.matrix
?matrix
# Extracting NHD Data ----
## Break Down Each Water Body and Flowline Based on FCode (determined by NHD)
### Dummy Variables
NamesWB = c("playa", "icemass", "LakePond", "reservoir",
"swamp", "estuary", "CanalDitch", "IntermittentStream",
"PerennialStream", "EphemeralStream")
# Extracting NHD Data ----
## Break Down Each Water Body and Flowline Based on FCode (determined by NHD)
### Dummy Variables
NamesWB = as.string(c("playa", "icemass", "LakePond", "reservoir",
"swamp", "estuary", "CanalDitch", "IntermittentStream",
"PerennialStream", "EphemeralStream"))
